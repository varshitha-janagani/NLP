{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLMrFtvWgNbKjbVC4rcs+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshitha-janagani/NLP/blob/main/2403a54085_nlp_ass_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUo3SWp-Bgoy",
        "outputId": "d9fdb8db-5dd8-4dbe-f53c-168e8757dbd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Documents:\n",
            "Doc 1: The football team celebrated a great win.\n",
            "Doc 2: The cricket team enjoyed a big victory.\n",
            "Doc 3: The government introduced a new policy.\n",
            "Doc 4: Doctors recommend daily exercise for health.\n",
            "Doc 5: Players trained hard for the match.\n",
            "Doc 6: The team prepared well for the game.\n",
            "Doc 7: Fans cheered loudly during the match.\n",
            "Doc 8: Regular exercise keeps the body strong.\n",
            "Doc 9: A balanced diet improves overall health.\n",
            "Doc 10: Healthy food supports good lifestyle habits.\n",
            "Doc 11: Fitness programs encourage active living.\n",
            "Doc 12: Artificial intelligence supports smart innovation.\n",
            "Doc 13: Exercise and diet support good health.\n",
            "Doc 14: Healthy habits improve daily lifestyle.\n",
            "Doc 15: The team celebrated another great victory.\n",
            "Doc 16: Computers help people solve problems quickly.\n",
            "Doc 17: Digital innovation supports better communication.\n",
            "Doc 18: Computers help people solve problems quickly.\n",
            "Doc 19: Smart devices make life easier for users.\n",
            "Doc 20: Digital innovation supports better communication.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The Football Team celebrated 2 Big Wins!!!\n",
            "Processed: ['football', 'team', 'celebrated', 'big', 'win']\n"
          ]
        }
      ],
      "source": [
        "D1 = \"The football team celebrated a great win.\"\n",
        "D2 = \"The cricket team enjoyed a big victory.\"\n",
        "D3 = \"The government introduced a new policy.\"\n",
        "D4 = \"Doctors recommend daily exercise for health.\"\n",
        "D5= \"Players trained hard for the match.\"\n",
        "D6= \"The team prepared well for the game.\"\n",
        "D7= \"Fans cheered loudly during the match.\"\n",
        "D8=\"Regular exercise keeps the body strong.\"\n",
        "D9= \"A balanced diet improves overall health.\"\n",
        "D10=\"Healthy food supports good lifestyle habits.\"\n",
        "D11=\"Fitness programs encourage active living.\"\n",
        "D12=\"Artificial intelligence supports smart innovation.\"\n",
        "D13=\"Exercise and diet support good health.\"\n",
        "D14=\"Healthy habits improve daily lifestyle.\"\n",
        "D15=\"The team celebrated another great victory.\"\n",
        "D16=\"Computers help people solve problems quickly.\"\n",
        "D17=\"Digital innovation supports better communication.\"\n",
        "D18=\"Computers help people solve problems quickly.\"\n",
        "D19=\"Smart devices make life easier for users.\"\n",
        "D20=\"Digital innovation supports better communication.\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,D16,D17,D18,D19,D20\n",
        "]\n",
        "\n",
        "print(\"Sample Documents:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Doc {i}: {doc}\")\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added as suggested by the error message\n",
        "\n",
        "# 1. Lowercasing\n",
        "def to_lowercase(text):\n",
        "    \"\"\"\n",
        "    Convert all characters to lowercase.\n",
        "    Purpose: Ensures uniformity, so 'Football' and 'football' are treated the same.\n",
        "    \"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "# 2. Remove punctuation & numbers\n",
        "def remove_punct_numbers(text):\n",
        "    \"\"\"\n",
        "    Remove punctuation and digits using regex.\n",
        "    Purpose: Keeps only meaningful words, avoids noise from symbols/numbers.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "# 3. Tokenization\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into individual words (tokens).\n",
        "    Purpose: Allows word-level analysis for similarity measures.\n",
        "    \"\"\"\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# 4. Remove stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"\n",
        "    Remove common words like 'the', 'is', 'and'.\n",
        "    Purpose: These words don’t add much meaning and can distort similarity.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# 5. (Optional) Lemmatization\n",
        "def lemmatize(tokens):\n",
        "    \"\"\"\n",
        "    Reduce words to their base form (lemma).\n",
        "    Example: 'running' → 'run', 'better' → 'good'.\n",
        "    Purpose: Groups word variants together for better semantic matching.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Combined preprocessing pipeline\n",
        "def preprocess(text, do_lemmatize=True):\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_punct_numbers(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    if do_lemmatize:\n",
        "        tokens = lemmatize(tokens)\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"The Football Team celebrated 2 Big Wins!!!\"\n",
        "print(\"Original:\", sample_text)\n",
        "print(\"Processed:\", preprocess(sample_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample dataset (first few documents)\n",
        "documents = [\n",
        "    D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,D16,D17,D18,D19,D20\n",
        "]\n",
        "\n",
        "# Create Bag-of-Words representation\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Show feature names (unique words)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Show Bag-of-Words matrix\n",
        "print(\"\\nBag-of-Words Matrix:\")\n",
        "print(bow_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKFXWBMOCEAT",
        "outputId": "0bbed21a-68d6-4562-961c-19e6ad2186bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['active' 'artificial' 'balanced' 'better' 'big' 'body' 'celebrated'\n",
            " 'cheered' 'communication' 'computers' 'cricket' 'daily' 'devices' 'diet'\n",
            " 'digital' 'doctors' 'easier' 'encourage' 'enjoyed' 'exercise' 'fans'\n",
            " 'fitness' 'food' 'football' 'game' 'good' 'government' 'great' 'habits'\n",
            " 'hard' 'health' 'healthy' 'help' 'improve' 'improves' 'innovation'\n",
            " 'intelligence' 'introduced' 'keeps' 'life' 'lifestyle' 'living' 'loudly'\n",
            " 'make' 'match' 'new' 'overall' 'people' 'players' 'policy' 'prepared'\n",
            " 'problems' 'programs' 'quickly' 'recommend' 'regular' 'smart' 'solve'\n",
            " 'strong' 'support' 'supports' 'team' 'trained' 'users' 'victory' 'win']\n",
            "\n",
            "Bag-of-Words Matrix:\n",
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset (8 documents for demonstration)\n",
        "documents = [\n",
        "    D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,D16,D17,D18,D19,D20\n",
        "]\n",
        "\n",
        "# Step 5 — Bag-of-Words representation\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 6 — Compute Cosine Similarity\n",
        "cosine_sim = cosine_similarity(bow_matrix, bow_matrix)\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "cosine_df = pd.DataFrame(cosine_sim,\n",
        "                         index=[f\"Doc{i}\" for i in range(1, len(documents)+1)],\n",
        "                         columns=[f\"Doc{i}\" for i in range(1, len(documents)+1)])\n",
        "\n",
        "print(\"Cosine Similarity Matrix (Bag-of-Words):\")\n",
        "print(cosine_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_A3uRUCTxT",
        "outputId": "000cf0a5-3710-4e5f-bab6-442668f86f9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Matrix (Bag-of-Words):\n",
            "           Doc1      Doc2  Doc3  Doc4  Doc5      Doc6  Doc7  Doc8  Doc9  \\\n",
            "Doc1   1.000000  0.200000   0.0   0.0  0.00  0.258199  0.00   0.0   0.0   \n",
            "Doc2   0.200000  1.000000   0.0   0.0  0.00  0.258199  0.00   0.0   0.0   \n",
            "Doc3   0.000000  0.000000   1.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc4   0.000000  0.000000   0.0   1.0  0.00  0.000000  0.00   0.2   0.2   \n",
            "Doc5   0.000000  0.000000   0.0   0.0  1.00  0.000000  0.25   0.0   0.0   \n",
            "Doc6   0.258199  0.258199   0.0   0.0  0.00  1.000000  0.00   0.0   0.0   \n",
            "Doc7   0.000000  0.000000   0.0   0.0  0.25  0.000000  1.00   0.0   0.0   \n",
            "Doc8   0.000000  0.000000   0.0   0.2  0.00  0.000000  0.00   1.0   0.0   \n",
            "Doc9   0.000000  0.000000   0.0   0.2  0.00  0.000000  0.00   0.0   1.0   \n",
            "Doc10  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc11  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc12  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc13  0.000000  0.000000   0.0   0.4  0.00  0.000000  0.00   0.2   0.4   \n",
            "Doc14  0.000000  0.000000   0.0   0.2  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc15  0.670820  0.447214   0.0   0.0  0.00  0.288675  0.00   0.0   0.0   \n",
            "Doc16  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc17  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc18  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc19  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "Doc20  0.000000  0.000000   0.0   0.0  0.00  0.000000  0.00   0.0   0.0   \n",
            "\n",
            "          Doc10  Doc11     Doc12     Doc13     Doc14     Doc15  Doc16  \\\n",
            "Doc1   0.000000    0.0  0.000000  0.000000  0.000000  0.670820    0.0   \n",
            "Doc2   0.000000    0.0  0.000000  0.000000  0.000000  0.447214    0.0   \n",
            "Doc3   0.000000    0.0  0.000000  0.000000  0.000000  0.000000    0.0   \n",
            "Doc4   0.000000    0.0  0.000000  0.400000  0.200000  0.000000    0.0   \n",
            "Doc5   0.000000    0.0  0.000000  0.000000  0.000000  0.000000    0.0   \n",
            "Doc6   0.000000    0.0  0.000000  0.000000  0.000000  0.288675    0.0   \n",
            "Doc7   0.000000    0.0  0.000000  0.000000  0.000000  0.000000    0.0   \n",
            "Doc8   0.000000    0.0  0.000000  0.200000  0.000000  0.000000    0.0   \n",
            "Doc9   0.000000    0.0  0.000000  0.400000  0.000000  0.000000    0.0   \n",
            "Doc10  1.000000    0.0  0.182574  0.182574  0.547723  0.000000    0.0   \n",
            "Doc11  0.000000    1.0  0.000000  0.000000  0.000000  0.000000    0.0   \n",
            "Doc12  0.182574    0.0  1.000000  0.000000  0.000000  0.000000    0.0   \n",
            "Doc13  0.182574    0.0  0.000000  1.000000  0.000000  0.000000    0.0   \n",
            "Doc14  0.547723    0.0  0.000000  0.000000  1.000000  0.000000    0.0   \n",
            "Doc15  0.000000    0.0  0.000000  0.000000  0.000000  1.000000    0.0   \n",
            "Doc16  0.000000    0.0  0.000000  0.000000  0.000000  0.000000    1.0   \n",
            "Doc17  0.182574    0.0  0.400000  0.000000  0.000000  0.000000    0.0   \n",
            "Doc18  0.000000    0.0  0.000000  0.000000  0.000000  0.000000    1.0   \n",
            "Doc19  0.000000    0.0  0.182574  0.000000  0.000000  0.000000    0.0   \n",
            "Doc20  0.182574    0.0  0.400000  0.000000  0.000000  0.000000    0.0   \n",
            "\n",
            "          Doc17  Doc18     Doc19     Doc20  \n",
            "Doc1   0.000000    0.0  0.000000  0.000000  \n",
            "Doc2   0.000000    0.0  0.000000  0.000000  \n",
            "Doc3   0.000000    0.0  0.000000  0.000000  \n",
            "Doc4   0.000000    0.0  0.000000  0.000000  \n",
            "Doc5   0.000000    0.0  0.000000  0.000000  \n",
            "Doc6   0.000000    0.0  0.000000  0.000000  \n",
            "Doc7   0.000000    0.0  0.000000  0.000000  \n",
            "Doc8   0.000000    0.0  0.000000  0.000000  \n",
            "Doc9   0.000000    0.0  0.000000  0.000000  \n",
            "Doc10  0.182574    0.0  0.000000  0.182574  \n",
            "Doc11  0.000000    0.0  0.000000  0.000000  \n",
            "Doc12  0.400000    0.0  0.182574  0.400000  \n",
            "Doc13  0.000000    0.0  0.000000  0.000000  \n",
            "Doc14  0.000000    0.0  0.000000  0.000000  \n",
            "Doc15  0.000000    0.0  0.000000  0.000000  \n",
            "Doc16  0.000000    1.0  0.000000  0.000000  \n",
            "Doc17  1.000000    0.0  0.000000  1.000000  \n",
            "Doc18  0.000000    1.0  0.000000  0.000000  \n",
            "Doc19  0.000000    0.0  1.000000  0.000000  \n",
            "Doc20  1.000000    0.0  0.000000  1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset (8 documents)\n",
        "documents = [\n",
        "    D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,D16,D17,D18,D19,D20\n",
        "]\n",
        "\n",
        "# Tokenize documents into sets of words\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1, set2 = set(doc1.lower().split()), set(doc2.lower().split())\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "# Compute Jaccard similarity matrix\n",
        "n = len(documents)\n",
        "jaccard_matrix = [[jaccard_similarity(documents[i], documents[j]) for j in range(n)] for i in range(n)]\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "jaccard_df = pd.DataFrame(jaccard_matrix,\n",
        "                          index=[f\"Doc{i}\" for i in range(1, n+1)],\n",
        "                          columns=[f\"Doc{i}\" for i in range(1, n+1)])\n",
        "\n",
        "print(\"Jaccard Similarity Matrix:\")\n",
        "print(jaccard_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU8sBp0sCefK",
        "outputId": "65347cd1-0d18-4138-cc94-c876cf0e480e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity Matrix:\n",
            "           Doc1      Doc2      Doc3      Doc4      Doc5      Doc6      Doc7  \\\n",
            "Doc1   1.000000  0.272727  0.181818  0.000000  0.083333  0.181818  0.083333   \n",
            "Doc2   0.272727  1.000000  0.181818  0.000000  0.083333  0.181818  0.083333   \n",
            "Doc3   0.181818  0.181818  1.000000  0.000000  0.090909  0.090909  0.090909   \n",
            "Doc4   0.000000  0.000000  0.000000  1.000000  0.090909  0.090909  0.000000   \n",
            "Doc5   0.083333  0.083333  0.090909  0.090909  1.000000  0.200000  0.200000   \n",
            "Doc6   0.181818  0.181818  0.090909  0.090909  0.200000  1.000000  0.090909   \n",
            "Doc7   0.083333  0.083333  0.090909  0.000000  0.200000  0.090909  1.000000   \n",
            "Doc8   0.083333  0.083333  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
            "Doc9   0.083333  0.083333  0.090909  0.090909  0.000000  0.000000  0.000000   \n",
            "Doc10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc13  0.000000  0.000000  0.000000  0.200000  0.000000  0.000000  0.000000   \n",
            "Doc14  0.000000  0.000000  0.000000  0.100000  0.000000  0.000000  0.000000   \n",
            "Doc15  0.444444  0.300000  0.090909  0.000000  0.090909  0.200000  0.090909   \n",
            "Doc16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Doc19  0.000000  0.000000  0.000000  0.083333  0.083333  0.083333  0.000000   \n",
            "Doc20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "           Doc8      Doc9     Doc10  Doc11     Doc12     Doc13  Doc14  \\\n",
            "Doc1   0.083333  0.083333  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc2   0.083333  0.083333  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc3   0.090909  0.090909  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc4   0.090909  0.090909  0.000000    0.0  0.000000  0.200000    0.1   \n",
            "Doc5   0.090909  0.000000  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc6   0.090909  0.000000  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc7   0.090909  0.000000  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc8   1.000000  0.000000  0.000000    0.0  0.000000  0.090909    0.0   \n",
            "Doc9   0.000000  1.000000  0.000000    0.0  0.000000  0.200000    0.0   \n",
            "Doc10  0.000000  0.000000  1.000000    0.0  0.100000  0.090909    0.1   \n",
            "Doc11  0.000000  0.000000  0.000000    1.0  0.000000  0.000000    0.0   \n",
            "Doc12  0.000000  0.000000  0.100000    0.0  1.000000  0.000000    0.0   \n",
            "Doc13  0.090909  0.200000  0.090909    0.0  0.000000  1.000000    0.0   \n",
            "Doc14  0.000000  0.000000  0.100000    0.0  0.000000  0.000000    1.0   \n",
            "Doc15  0.090909  0.000000  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc16  0.000000  0.000000  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc17  0.000000  0.000000  0.100000    0.0  0.111111  0.000000    0.0   \n",
            "Doc18  0.000000  0.000000  0.000000    0.0  0.000000  0.000000    0.0   \n",
            "Doc19  0.000000  0.000000  0.000000    0.0  0.090909  0.000000    0.0   \n",
            "Doc20  0.000000  0.000000  0.100000    0.0  0.111111  0.000000    0.0   \n",
            "\n",
            "          Doc15  Doc16     Doc17  Doc18     Doc19     Doc20  \n",
            "Doc1   0.444444    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc2   0.300000    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc3   0.090909    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc4   0.000000    0.0  0.000000    0.0  0.083333  0.000000  \n",
            "Doc5   0.090909    0.0  0.000000    0.0  0.083333  0.000000  \n",
            "Doc6   0.200000    0.0  0.000000    0.0  0.083333  0.000000  \n",
            "Doc7   0.090909    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc8   0.090909    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc9   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc10  0.000000    0.0  0.100000    0.0  0.000000  0.100000  \n",
            "Doc11  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc12  0.000000    0.0  0.111111    0.0  0.090909  0.111111  \n",
            "Doc13  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc14  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc15  1.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
            "Doc16  0.000000    1.0  0.000000    1.0  0.000000  0.000000  \n",
            "Doc17  0.000000    0.0  1.000000    0.0  0.000000  1.000000  \n",
            "Doc18  0.000000    1.0  0.000000    1.0  0.000000  0.000000  \n",
            "Doc19  0.000000    0.0  0.000000    0.0  1.000000  0.000000  \n",
            "Doc20  0.000000    0.0  1.000000    0.0  0.000000  1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Dataset\n",
        "documents = [\n",
        "    \"The football team celebrated a great win.\",\n",
        "    \"The cricket team enjoyed a big victory.\",\n",
        "    \"The government introduced a new policy.\",\n",
        "    \"Doctors recommend daily exercise for health.\",\n",
        "    \"Players trained hard for the match.\",\n",
        "    \"The team prepared well for the game.\",\n",
        "    \"Fans cheered loudly during the match.\",\n",
        "    \"Regular exercise keeps the body strong.\",\n",
        "    \"A balanced diet improves overall health.\",\n",
        "    \"Healthy food supports good lifestyle habits.\",\n",
        "    \"Fitness programs encourage active living.\",\n",
        "    \"Artificial intelligence supports smart innovation.\",\n",
        "    \"Exercise and diet support good health.\",\n",
        "    \"Healthy habits improve daily lifestyle.\",\n",
        "    \"The team celebrated another great victory.\",\n",
        "    \"Computers help people solve problems quickly.\",\n",
        "    \"Digital innovation supports better communication.\",\n",
        "    \"Computers help people solve problems quickly.\",\n",
        "    \"Smart devices make life easier for users.\",\n",
        "    \"Digital innovation supports better communication.\"\n",
        "]\n",
        "\n",
        "# Preprocessing: tokenize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "\n",
        "# WordNet-based similarity between two sentences\n",
        "def sentence_similarity(sent1, sent2, method=\"wup\"):\n",
        "    words1 = preprocess(sent1)\n",
        "    words2 = preprocess(sent2)\n",
        "\n",
        "    synsets1 = [wn.synsets(w)[0] for w in words1 if wn.synsets(w)]\n",
        "    synsets2 = [wn.synsets(w)[0] for w in words2 if wn.synsets(w)]\n",
        "\n",
        "    if not synsets1 or not synsets2:\n",
        "        return 0\n",
        "\n",
        "    scores = []\n",
        "    for s1 in synsets1:\n",
        "        best_score = max((s1.wup_similarity(s2) if method==\"wup\" else s1.path_similarity(s2)) or 0 for s2 in synsets2)\n",
        "        scores.append(best_score)\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# Example: compute similarity for first 10 pairs\n",
        "pairs = [(documents[i], documents[i+1]) for i in range(10)]\n",
        "for i, (d1, d2) in enumerate(pairs, 1):\n",
        "    sim = sentence_similarity(d1, d2, method=\"wup\")\n",
        "    print(f\"Pair {i}: {sim:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRhddijRCsWI",
        "outputId": "f2cd7a14-a015-4326-bd80-c6b6aaf80d56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pair 1: 0.695\n",
            "Pair 2: 0.445\n",
            "Pair 3: 0.242\n",
            "Pair 4: 0.346\n",
            "Pair 5: 0.431\n",
            "Pair 6: 0.367\n",
            "Pair 7: 0.457\n",
            "Pair 8: 0.350\n",
            "Pair 9: 0.436\n",
            "Pair 10: 0.530\n"
          ]
        }
      ]
    }
  ]
}